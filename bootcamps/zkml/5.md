### ZKML Q&A

**Q: How much performance loss using ZoKrates vs. Circom?**
**A:** ZoKrates slower (e.g., 1min for zk-email vs. 15s RSA in Circom). Circom optimized for R1CS/SNARKs; ZoKrates higher-level but less efficient for large circuits.

**Q: Comparison between GKR protocol and Circom?**
**A:** GKR: Interactive proof for circuit evaluation (linear prover, sublinear verifier). Circom: DSL compiling to R1CS for SNARKs/STARKs. GKR for aggregation; Circom for circuit construction.

**Q: How important is legible circuit code comparable to non-circuit ML code?**
**A:** Essential for auditing/verification; ensures correctness without ZK abstraction hiding bugs.

**Q: Uses of ZK on evasion attacks? E.g., white-hat hacker for ML bug bounty?**
**A:** ZK proves model robustness without revealing attack vectors; enables verifiable evasion tests for bounties.

**Q: Good idea for OpenAI to use ZKML on inference to prove correct model use?**
**A:** Yes; proves exact model execution without parameter reveal, ensuring integrity.

**Q: Viable/useful to provide model as private input in ZKML?**
**A:** Yes; hides proprietary models while proving inference correctness.

**Q: How is ZKML use in healthcare different from federated learning?**
**A:** ZKML proves computation privacy (no data sharing); FL aggregates models without raw data. ZKML for verifiable inference; FL for collaborative training.

**Q: Why did Web3 fail in supply chain? Privacy reasons?**
**A:** Privacy exposure via transparency; plus complex integration, regulations, scalability issues.

**Q: What is the signature for in the attested microphone map?**
**A:** Hardware ECDSA signature links audio to device; ZK-SNARKs extend trust for edits.

**Q: With LLM parameters expanding, ZKML verifies computation but not bias—won't catch up?**
**A:** ZKML verifies execution; bias needs separate tools. Value in privacy/integrity despite growth.

**Q: Know TensorFlow ZKML applications? Mystique in Rosetta framework? Used in projects?**
**A:** Yes; Mystique enables ZK conversions for ML in Rosetta (TensorFlow privacy). Used for secure inference (e.g., ResNet).

**Q: EZKL demo 130 MB PK for 1-parameter model! Size for 8M model?**
**A:** Scales with complexity; 8M params could reach GBs. Optimize via quantization.

**Q: Comment on privy.io embedded wallets?**
**A:** Privy.io embedded wallets use ZK-based authentication; great for UX/security.

**Q: What is quantization? Like rounding up/down?**
**A:** Maps floats to fixed-point/integers (e.g., discrete levels) for ZK efficiency.

**Q: What is lookup argument? Costly to keep all input/output pairs?**
**A:** Verifies values in precomputed tables efficiently (e.g., Plookup). Large tables costly, optimized via commitments/logarithmic checks.

**Q: Does output column contain gates & final outputs?**
**A:** Output column holds final results; gates define operations.

**Q: Do neurons use floating-point? FP representation in finite fields?**
**A:** Yes; approximated as fixed-point/scaled integers to fit fields.

**Q: Is Groth16 still frequently used? Will it continue?**
**A:** Yes, for efficiency (small proofs, fast verification). Continues where trusted setup acceptable.

**Q: ZK with floats? Impossible?**
**A:** Not impossible; approximated as fixed-point/integers with scaling.

**Q: What is weightless approach?**
**A:** Proves inference without weights as inputs, reducing circuit complexity.

**Q: Costly range proofs for wide ranges challenge activation functions with thresholds?**
**A:** Yes; use optimized circuits/lookup tables for thresholds.

**Q: Plonk based on KZG? Requires trusted setup?**
**A:** Plonk uses KZG PCS by default (trusted setup); variants like TurboPlonk avoid it.

**Q: How can random oracle access model be random with underlying seed?**
**A:** Pseudo-random; seed ensures determinism while simulating randomness.

**Q: How verifier satisfied with partial proof/commitment?**
**A:** PCS (e.g., KZG) ensures partial evaluations represent full polynomial.

**Q: How secret key kept from prover in hash function?**
**A:** Prover uses public hash; secret key not hashed—used for signing/verification.

**Q: Need different proof for each input?**
**A:** Yes, unique proof per input; reusable for same computation.

**Q: Why EZKL uses Halo2? Advantages over others?**
**A:** Halo2: Recursive, no trusted setup, constant proofs, efficient verification. Better for ML non-linearities via lookups.

**Q: How prevent service provider using different models if private?**
**A:** ZK proves exact model execution; commit to model hash/parameters.

**Q: Size of benchmark model?**
**A:** MNIST classifier: ~784 input nodes, simple layers (e.g., 128 hidden, 10 output); ~10k-100k parameters.

**Q: How EZKL compares to RISC Zero? Run arbitrary Rust program?**
**A:** EZKL: ML-focused (ONNX to Halo2 circuits), lower memory (98% less than RISC Zero). RISC Zero: zkVM for arbitrary Rust/RISC-V, higher memory but general-purpose.

**Q: Does Groth16 use KZG?**
**A:** No, Groth16 uses pairing-based QAP; KZG is for PLONK/Halo2.

**Q: Model (784 units layer 1) for image input?**
**A:** Yes, MNIST: 784 pixels (28x28) as input layer.

**Q: With 2 proofs, know if same model?**
**A:** Yes, commit to model hash/parameters; matching commitments confirm same model.

**Q: Link to Colab and Remix playground?**
**A:** Colab: [MNIST Classifier](https://colab.research.google.com/github/zkonduit/ezkl/blob/main/examples/notebooks/mnist_classifier.ipynb). Remix: Load ZoKrates plugin at [remix.ethereum.org](https://remix.ethereum.org); no direct EZKL integration—use ONNX export.

**Q: Concept of proving accuracy if known from test data? Train until accuracy threshold instead of epochs?**
**A:** Proves verifiable accuracy on private data; threshold training viable for convergence.

**Q: EZKL options for public/private weights/inputs?**
**A:** Yes; configurable: weights public/private, inputs public/private.

**Q: Google Drive folder?**
**A:** [Drive Folder](https://drive.google.com/drive/folders/1CeJUWD_mdKj7ziaHu_XU-yUfak1KYeQA).

**Q: Public inputs in SNARK backend prover vs. private?**
**A:** Public: Verifiable outputs, commitments (e.g., model hash, final predictions). Private: Sensitive inputs (data, weights), intermediate computations.

**Q: Verkle trees use polynomial commitment schemes?**
**A:** Yes, KZG-based for vector commitments.

**Q: Hash, pairing group, discrete log, UO, class, RSA group?**
**A:** Hash: One-way function (e.g., SHA256). Pairing group: Elliptic curves for bilinear maps (SNARKs). Discrete log: Hard problem underlying EC security. UO: Universal optimizer? Class: Math structure. RSA group: Modular arithmetic for RSA.

**Q: For project, stick to Solidity?**
**A:** Yes, for Ethereum compatibility; ZKML proofs verifiable on-chain.

**Q: Weights hardcoded into circuit or input?**
**A:** Input (public/private); hardcoded increases size, reduces flexibility.

**Q: Where in EZKL code floating-point to finite field conversion?**
**A:** In quantization phase (ONNX to circuit); scales/truncates floats to integers.

**Q: Gas cost for single on-chain verification in EZKL example?**
**A:** 200k-500k gas for pairing-heavy proofs (~$20 at current prices).
